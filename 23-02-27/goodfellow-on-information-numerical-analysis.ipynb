{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* reading goodfellow's \n",
    "    * end of 3rd - very gentle information theory\n",
    "    * 4th - numerical analysis for machine learning\n",
    "    * start of 5th - machine learning presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* doing a quick wrap-up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for **information theory**, \n",
    "    * he starts by a nice informal definition of the entropy of an r.v., which has to do with how much information one gets out of knowing its value\n",
    "    * then gives the desideratum given by shannon, and it's formal definition\n",
    "    * some discussion on measuring relative information with KL-divergence\n",
    "    * and writing KL-divergence as a function of cross-entropy - we hope, to introduce variational inference somewhen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for **numerical analysis**,\n",
    "    * he goes for describing the relevance of underflow and overflow in machine learning algorithms\n",
    "    * then goes by the steepest descent algorithm, relating its motivation with taylor expansions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lew",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "737a60cdb60ce0e7b2cbb61a832f7f77656e2305e838eae61f4045b6f779a0b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
